{
  
    
        "post0": {
            "title": "Modern NLP with Little to No Annotated Data",
            "content": "Natural language processing is a task-rich area and enumerating every type of low-resource learning technique for all likely tasks of interest is impractical. Instead, I will focus primarily on sequence classification but describe techniques applicable to a wide variety of data (in)availability situations. My hope is that these methods will be useful for some and, for most, will inspire creativity in leveraging pre-trained models in low-resource settings. . Settings (focus on classification): . No annotated data is available NLI Model | SBERT2Wordvec | . | Data is available for some labels, but missing for others NLI Model | Align SBERT2Wordvec | . | No annotated data is available, but lots of non-annotated Semi-supervised... be careful not to scoop your own research here | . | Some annotated data is available, but not enough to learn a good classifier Few shot, sample efficiency | . | Data is available, but not in the language I want Cross lingual alignment techniques, link to seb ruder&#39;s post | . | . Setting: No Training Data is Available . Background: Natural Language Inference (NLI) . Several of the methods described below use Natural Language Inference as pre-training step, so here is a quick review. NLI considers two sentences: a &quot;premise&quot; and a &quot;hypothesis&quot;. The task is to determine whether the hypothesis is true (entailment) or false (contradiction) given the premise. . . When using transformer architectures like BERT, NLI datasets are typically modeled via sequence-pair classification. That is, we feed both the premise and the hypothesis through the model together as distinct segments and learn a classification head predicting one of [contradiction, neutral, entailment]. . A ready-made zero-shot classifier . Recently, Yin et al. (2019) proposed a method which uses a pre-trained MNLI sequence-pair classifier as an out-of-the-box zero-shot text classifier that actually works pretty well. . The idea is to take the sequence we&#39;re interested in labeling as the &quot;premise&quot; and to turn each candidate label into a &quot;hypothesis.&quot; If the model says that the premise &quot;entails&quot; the hypothesis, we take the label to be true. This gives us a ready-made compatibility function that works reasonably well on certain tasks without any task-specific training. See the code snippet below to see how easily this can be done with 🤗 Transformers. . #collapse-show # load model pretrained on MNLI from transformers import BartForSequenceClassification, BartTokenizer tokenizer = BartTokenizer.from_pretrained(&#39;bart-large-mnli&#39;) model = BartForSequenceClassification.from_pretrained(&#39;bart-large-mnli&#39;) # pose sequence as a NLI premise and label (politics) as a hypothesis premise = &#39;Who are you voting for in 2020?&#39; hypothesis = &#39;This text is about politics.&#39; # run through model pre-trained on MNLI input_ids = tokenizer.encode(premise, hypothesis, return_tensors=&#39;pt&#39;) logits = model(input_ids)[0] # we throw away &quot;neutral&quot; (dim 1) and take the probability of # &quot;entailment&quot; (2) as the probability of the label being true entail_contradiction_logits = logits[:,[0,2]] probs = entail_contradiction_logits.softmax(dim=1) true_prob = probs[:,1].item() * 100 print(f&#39;Probability that the label is true: {true_prob:0.2f}%&#39;) . . Probability that the label is true: 99.04% . In the paper, the authors report an F1 of $37.9$ on Yahoo Answers using the smallest version of BERT fine-tuned only on the Multi-genre NLI (MNLI) corpus. By simply using the larger and more recent Bart model pre-trained on MNLI, we were able to bring this number up to $53.7$. For context, Yahoo Answers has 10 classes and supervised models get an accuracy of just over $70 %$. . Of course, this number can be improved when some data is available for training. In addition to the extreme fully unsupervised setting, the authors consider a setup which corresponds to the traditional generalized zero-shot learning setting where only a subset of the dataset&#39;s labels are available during training. The model is then evaluated on all labels together, both seen and unseen, at test time. . See our live demo here to try it out for yourself! Enter a sequence you want to classify and any labels of interest and watch Bart do its magic in real time. . . A Latent Embedding Approach . A slightly less effective but more flexible approach is to embed both the sequence and the class names of interest into the same representation space and then simply select the label closest in latent space. . This is a well-known technique in Zero Shot Learning in Computer Vision. Take the word vectors for each class and some latent representation for an image and project them to the same space. Learning this projection requires data for some labels, but allows you to generalize to unseen labels at test time. . We found that in the text regime, we can follow the same procedure but without the need for any annotated data ahead of time. By simply using a single sentence representation model, we can embed both the sequences to classify and the candidate labels into the same latent space. . $$ hat{c} = arg max_{c in C} cos( Phi(x), Phi(c)) $$where $ Phi$ is a sentence-level embedding model, $x$ is a sequence, and $C$ is a set of class labels. . Here&#39;s an example code snippet using the Sentence-BERT method: . # load the sentence-bert model from the HuggingFace model hub from transformers import AutoTokenizer, AutoModel from torch.nn import functional as F tokenizer = AutoTokenizer.from_pretrained(&quot;deepset/sentence_bert&quot;) model = AutoModel.from_pretrained(&quot;deepset/sentence_bert&quot;) sentence = &#39;Who are you voting for in 2020?&#39; labels = [&#39;politics&#39;, &#39;business&#39;, &#39;art &amp; culture&#39;] # run inputs through model and mean over the sequence dimension # to get sentence-level representations inputs = tokenizer.batch_encode_plus([sentence] + labels, return_tensors=&#39;pt&#39;, pad_to_max_length=True) input_ids = inputs[&#39;input_ids&#39;] attention_mask = inputs[&#39;attention_mask&#39;] output = model(input_ids, attention_mask=attention_mask)[0] sentence_rep = output[:1].mean(dim=1) label_reps = output[1:].mean(dim=1) # now find the labels with the highest cosine similarities to # the sentence similarities = F.cosine_similarity(sentence_rep, label_reps) closest = similarities.argsort(descending=True) for ind in closest: print(f&#39;label: {labels[ind]}, t cos: {similarities[ind]:0.4f}&#39;) . label: politics, cos: 0.2156 label: business, cos: 0.0045 label: art &amp; culture, cos: -0.0274 . One downside to this method is that Sentence-BERT is designed to learn sentence-level, not single- or multi-word representations like our class names. It is therefore reasonable to suppose that our label embeddings may not be as semantically salient as popular word-level embedding methods. But we can&#39;t use word vectors directly because then we would have to align our S-BERT sequence embeddings with the word vector label emebddings. . We addressed this issue with the following procedure: . Take the top $10,000$ most frequent words $V$ in skipgram&#39;s vocabulary | Obtain embeddings for each word using skipgram $ Phi_{ text{word}}(V)$ | Obtain embeddings for each word using S-BERT $ Phi_{ text{sent}}(V)$ | Learn a linear projection $W$ with L2 regularization from $ Phi_{ text{sent}}(V)$ to $ Phi_{ text{word}}(V)$ | Now we apply $W$ as an additional transformation to our latent space for both sequence and label embeddings: . $$ hat{c} = arg max_{c in C} cos( Phi_{ text{sent}}(x)W, Phi_{ text{sent}}(c)W) $$ Setting: Low-Resource Languages . Low-resource and cross-lingual learning is a huge research area in NLP right now and much has been written about it, so I&#39;ll just link a few great resources: . Graham Neubig&#39;s recently released Low Resource NLP Bootcamp. At @LTIatCMU we held a week-long &quot;Low Resource Natural Language Processing Bootcamp&quot; with 8 sets of lectures &amp; exercises on getting NLP to work in languages where resources are less abundant. We&#39;re making them available for all who are interested here: https://t.co/GGc5tAM2lh 1/ pic.twitter.com/oJOmiyTyCS . &mdash; Graham Neubig (@gneubig) May 27, 2020 | Sebastian Ruder&#39;s blog post, &quot;A survey of cross-lingual word embedding models&quot; | .",
            "url": "https://joeddav.github.io/blog/2020/05/28/ZSL.html",
            "relUrl": "/2020/05/28/ZSL.html",
            "date": " • May 28, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "REALM: Knowledge and Transformers",
            "content": "Summary for the Hugging Face awesome-papers reading group, March 3, 2020. Paper: https://arxiv.org/abs/2002.08909. . Background: Language Models as Knowledge Bases . Due in large part to the massive size and scope of the text corpora on which they are trained, huge language representation learners like BERT have been shown to encode a surprising amount of world knowledge in their weights. A recent EMNLP paper posed the question of whether models like BERT can be thought of as having some form of latent knowledge base in their parameters. Models like BERT, and in particular T5, have been shown to do surprisingly well on open-domain question answering, a deliberately information-intensive task, despite having no access to external databases (incidentally, REALM shows how well we can do when such a model is given that access). All of this is to suggest the possibility that, given enough parameters and training data, models might be able to make external knowledge augmentation superflous, instead inferring relevant knowledge from text corpora and encoding it in its parameters. . Background: Instance-based Learning and Retrieve-and-edit Models . Instance- or memory-based learning is a family of ML algorithms which compare a data point with instances already seen in training (or existing in some reference set), rather than relying entirely on learned model parameters for generalization. An example of this class of algorithm is KNN, which looks up the most similar points in a training set to generalize to a new instance. Retrieve-and-edit can be thought of as a type of instance-based learning with two components: a retriever which chooses similar training examples to a given data point, and an editor which then modifies the retrieved examples to form an appropriate prediction. . Recently, KNN Language Models was proposed as an ICLR 2020 paper. This method looks up the nearest neighbors in LM-embedding space to generate target word predictions. The method does quite well in terms of perplexity, but the authors don’t evaluate on downstream task performance, and it’s questionable how amenable the method is to downstream fine-tuning at all. Regardless, it’s a great example of using instance-based learning to improve model performance. . Unlike KNN-LM, REALM incorporates retrieved instances by appending them to the model context, but it is not the first to propose such a method. Its direct precursor (published by one of REALM’s first authors), Latent Retrieval for Weakly Supervised Open Domain Question Answering, introduces a pre-training task for the corpus retriever which “makes it possible” to train end-to-end on Wikipedia. Much of the work presented here as novel actually builds upon work done in this prior publication. The specific contributions presented in the REALM paper largely involve pre-training tasks and computational tricks, ultimately yielding impressive empirical results on OpenQA. Other methods, such as Facebook’s DrQA, employ methods for retrieving knowledge from Wikipedia text as well. . REALM: Retrieval-Augmented Langauge Models . The authors propose a method for training a masked language model (MLM) by sparsely “attending” over all of Wikipedia in an end-to-end fashion. . . At a high level, the method goes like this: find the most similar text passages in BERT space, add those passages to the input as additional context, and then make a prediction. . Here’s the more formal, probabilistic explanation of their training objective: Suppose we have a corrupted sentence $x$ and hidden tokens $y$, as well as a textual knowledge corpus $ mathcal{Z}$ (i.e., Wikipedia articles). The objective involves marginalizing over the entire Wikipedia corpus: . begin{equation} p(y|x) = sum_{z in mathcal{Z}} p(y|x,z) p(z|x) end{equation} . Of course, summing over every document in Wikipedia is computationally impractical. In some cases we approximate things like this with Monte Carlo: begin{equation} p(y|x) approx frac{1}{K} sum_{z sim p(z|x)}^K p(y|x,z) end{equation} In other words, if we can sample $K$ documents from the conditional distribution $p(z|x)$ and sum over the resulting target likelihoods, we get an unbiased estimator of the objective. . In practice, the authors sample from Wikipedia by simply taking the top $K$ most similar documents to $x$. They did this because selecting the top $K$ allows them to use Maximum Inner Product Search (MIPS) for huge computational benefits, but at the cost of a biased approximation of the objective. . . The authors evaluate their model on the downstream task of open-domain question answering, comfortably outperforming all other evaluated systems. It should also be noted that most of the included benchmark methods (excluding T5) also use Wikipedia in some way for external information at test time. . . My 2¢ . I found this paper interesting because of its commentary on knowledge representation and instance-based language modeling. Do language models have latent knowledge bases? To what degree is that knowledge accessible? This paper makes the argument for “explicitly” modeling the relevant knowledge needed to perform a given task, rather than relying on inferred knowledge. However, the impressive performance on OpenQA benchmarks notwithstanding, they do nothing to substantiate their claims about improved interpretability and modularity of model predictions. For all we know, retrieval from Wikipedia could have increased the knowledge encoded in the model parameters, rather than decreased it. . It would also have been interesting to see more analysis on what exactly the model gets out of retrieved examples. Given that Wikipedia is a natural text corpus, is it possible that the model is attending to linguistic cues in retrieved examples in addition to factual information? The paper focused more on the computational aspect of things, which to be fair is arguably where their greatest contribution was, but I wish they had done some more analysis. . Discussion Questions . In the introduction, the authors state the following: “In contrast to models that store knowledge in their parameters, this approach explicitly exposes the role of world knowledge by asking the model to decide what knowledge to retrieve and use during inference.” In what way is is this type of knowledge-augmented model valuable compared to standard models which rely on the “latent knowledge” in their parameters? Will the future of NLP involve explicitly incorporating external knowledge, or will such methods become obsolete with bigger and better models? | Should Wikipedia be the go-to general “knowledge base”? Should we focus more on structured knowledge (such as WikiData, which is built from Wikipedia) or large text corpora? | The REALM objective involves marginalizing over the document corpus. The authors approximate this by summing over the predictions corresponding to the top-k highest scoring documents. Presumably, they take this approach (rather than coming up with a stochastic sampling method) to get the computational advantages of asynchronous MIPS. Is this biased estimator of the objective problematic or not a big deal? Why? | Is there a future for instance-based models like REALM in transformers? | In the discussion, the authors mention that there are several different lenses through which you can view their method: a different take on knowledge representation and augmentation; a transformer with much larger, sparsely-attended contexts; a memory-based or retrieve-and-edit model with learned retrieval. Do any of these perspectives particularly resonate? Are there others that you prefer? | Summary of HF Internal Discussion . When knowledge needs to be updated, models like T5 which encode knowledge in their weights must be retrained to reflect the new information. If the model relies on the knowledge base, theoretically all you have to do is update the knowledge base. That’s a major advantage for real-life deployed QA systems, for example. | Bootstrapping the retrieval model is a tough thing to do and the Inverse Cloze Task is a smart way to go about it. | Impossible to know whether the biased objective (Q3 above) has a negative impact on results without more analysis or experimentation, but it would have been nice if the authors discussed it more in the paper. | It would nice to see evaluation on tasks other than just OpenQA. | .",
            "url": "https://joeddav.github.io/blog/2020/03/03/REALM.html",
            "relUrl": "/2020/03/03/REALM.html",
            "date": " • Mar 3, 2020"
        }
        
    
  

  
  

  

  
  

  
  

  

  
  

  

  
      ,"page7": {
          "title": "",
          "content": "Warning . Do not manually save images into this folder. This is used by GitHub Actions to automatically copy images. Any images you save into this folder could be deleted at build time. .",
          "url": "https://joeddav.github.io/blog/images/copied_from_nb/",
          "relUrl": "/images/copied_from_nb/",
          "date": ""
      }
      
  

  
  

}