{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ZSL Blog Post.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoMVvIRtMXmY",
        "colab_type": "text"
      },
      "source": [
        "# An Overview of Zero Shot Learning in NLP\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aILhOF60WcIm",
        "colab_type": "text"
      },
      "source": [
        "### Introduction\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xC04NtlJWgyh",
        "colab_type": "text"
      },
      "source": [
        "### "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AB9T-krfQ9Ip",
        "colab_type": "text"
      },
      "source": [
        "### A Ready-made Zero-Shot Text Classifier\n",
        "\n",
        "Recently [Yin et al.](https://arxiv.org/abs/1909.00161) proposed a method which uses a pre-trained MNLI sequence-pair classifier as an out-of-the-box text/label compatibility function.\n",
        "\n",
        "Natural Language Inference (NLI) considers two sentences: a \"premise\" and a \"hypothesis\". The task is to determine whether the hypothesis is true (entailment) given the hypothesis. The following example comes from [NLP Progress](http://nlpprogress.com/english/natural_language_inference.html).\n",
        "\n",
        "|Premise|Label|Hypothesis|\n",
        "|-|-|-|\n",
        "|A man inspects the uniform of a figure in some East Asian country.\t|contradiction | The man is sleeping.|\n",
        "|An older and younger man smiling.\t | neutral | Two men are smiling and laughing at the cats playing on the floor. |\n",
        "|A soccer game with multiple males playing.\t|entailment | Some men are playing a sport.|\n",
        "\n",
        "The idea is to take the sequence as the \"premise\" and turn each possible label into a \"hypothesis.\" If the model says that the premise \"entails\" the hypothesis, we take the label to be true. This gives us a ready-made compatibility function that works reasonably well on certain tasks without any task-specific training. See the code snippet below to see how easily this can be done with ðŸ¤— Transformers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "La_ga8KvSFYd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "274cdb48-a518-4cee-c031-e27947e9a7ed"
      },
      "source": [
        "#collapse-show\n",
        "# load model pretrained on MNLI\n",
        "from transformers import BartForSequenceClassification, BartTokenizer\n",
        "tokenizer = BartTokenizer.from_pretrained('bart-large-mnli')\n",
        "model = BartForSequenceClassification.from_pretrained('bart-large-mnli')\n",
        "\n",
        "# pose sequence as a NLI premise and label (politics) as a hypothesis\n",
        "premise = \"Who are you voting for in 2020?\"\n",
        "hypothesis = 'This text is about politics.'\n",
        "\n",
        "# run through model pre-trained on MNLI\n",
        "input_ids = tokenizer.encode(premise, hypothesis, return_tensors='pt')\n",
        "logits = model(input_ids)[0]\n",
        "\n",
        "# we throw away \"neutral\" (dim 1) and take the probability of\n",
        "# \"entailment\" (2) as the probability of the label being true \n",
        "entail_contradiction_logits = logits[:,[0,2]]\n",
        "probs = entail_contradiction_logits.softmax(dim=1)\n",
        "true_prob = probs[:,1].item() * 100\n",
        "print(f'Probability that the label is true: {true_prob:0.2f}%')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Probability that the label is true: 99.04%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwXSg464flMJ",
        "colab_type": "text"
      },
      "source": [
        "In their paper, the authors report an F1 of 37.9 on Yahoo Answers using the smallest version of BERT fine-tuned only on the Multi-genre NLI (MNLI) corpus. By simply using the larger and more recent Bart model pre-trained on MNLI, we were able to bring this model up to NUMBER."
      ]
    }
  ]
}