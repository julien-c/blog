<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><link rel="shortcut icon" type="image/x-icon" href="/blog/assets/icons/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Modern NLP with Little to No Annotated Data | Joe Davison</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Modern NLP with Little to No Annotated Data" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Just a simple fastpages site for publishing things here and there" />
<meta property="og:description" content="Just a simple fastpages site for publishing things here and there" />
<link rel="canonical" href="https://joeddav.github.io/blog/2020/05/28/ZSL.html" />
<meta property="og:url" content="https://joeddav.github.io/blog/2020/05/28/ZSL.html" />
<meta property="og:site_name" content="Joe Davison" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-05-28T00:00:00-05:00" />
<script type="application/ld+json">
{"url":"https://joeddav.github.io/blog/2020/05/28/ZSL.html","@type":"BlogPosting","headline":"Modern NLP with Little to No Annotated Data","dateModified":"2020-05-28T00:00:00-05:00","datePublished":"2020-05-28T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://joeddav.github.io/blog/2020/05/28/ZSL.html"},"description":"Just a simple fastpages site for publishing things here and there","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  <link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
  <link rel="stylesheet" href="/blog/assets/main.css">
  <link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://joeddav.github.io/blog/feed.xml" title="Joe Davison" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
      document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
          delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
          ]}
        );
      });
    </script>
  

  <script>
  function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
  }
  window.onload = wrap_img;
  </script>

  <script>
    document.addEventListener("DOMContentLoaded", function(){
      // add link icon to anchor tags
      var elem = document.querySelectorAll(".anchor-link")
      elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
      // remove paragraph tags in rendered toc (happens from notebooks)
      var toctags = document.querySelectorAll(".toc-entry")
      toctags.forEach(e => (e.firstElementChild.innerText = e.firstElementChild.innerText.replace('Â¶', '')))
    });
  </script>
</head><body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">Joe Davison</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Modern NLP with Little to No Annotated Data</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-05-28T00:00:00-05:00" itemprop="datePublished">
        May 28, 2020
      </time>
       â€¢ <span class="read-time" title="Estimated read time">
    
    
      6 min read
    
</span></p>

    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">
<a href="https://github.com/joeddav/blog/tree/master/_notebooks/2020-05-28-ZSL.ipynb" role="button">
    <img class="notebook-badge-image" src="/blog/assets/badges/github.svg" alt="View On GitHub">
</a>
</div><div class="px-2">
    <a href="https://colab.research.google.com/github/joeddav/blog/blob/master/_notebooks/2020-05-28-ZSL.ipynb">
        <img class="notebook-badge-image" src="/blog/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-05-28-ZSL.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Natural language processing is a task-rich area and enumerating every type of low-resource learning technique for all likely tasks of interest is impractical. Instead, I will focus primarily on sequence classification but describe techniques applicable to a wide variety of data (in)availability situations. My hope is that these methods will be useful for some and, for most, will inspire creativity in leveraging pre-trained models in low-resource settings.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Settings (focus on classification):</p>
<ul>
<li>No annotated data is available<ul>
<li>NLI Model</li>
<li>SBERT2Wordvec</li>
</ul>
</li>
<li>Data is available for some labels, but missing for others<ul>
<li>NLI Model</li>
<li>Align SBERT2Wordvec</li>
</ul>
</li>
<li>No annotated data is available, but lots of non-annotated<ul>
<li>Semi-supervised... be careful not to scoop your own research here</li>
</ul>
</li>
<li>Some annotated data is available, but not enough to learn a good classifier<ul>
<li>Few shot, sample efficiency</li>
</ul>
</li>
<li>Data is available, but not in the language I want<ul>
<li>Cross lingual alignment techniques, link to seb ruder's post</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Setting:-No-Training-Data-is-Available">Setting: No Training Data is Available<a class="anchor-link" href="#Setting:-No-Training-Data-is-Available"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Background:-Natural-Language-Inference-(NLI)">Background: Natural Language Inference (NLI)<a class="anchor-link" href="#Background:-Natural-Language-Inference-(NLI)"> </a></h4><p>Several of the methods described below use Natural Language Inference as pre-training step, so here is a quick review. NLI considers two sentences: a "premise" and a "hypothesis". The task is to determine whether the hypothesis is true (entailment) or false (contradiction) given the premise.</p>
<p><img src="https://i.ibb.co/gWCjvdP/Screen-Shot-2020-05-26-at-5-10-07-PM.png" alt="example NLI sentences" title="Examples from http://nlpprogress.com/english/natural_language_inference.html" /></p>
<p>When using transformer architectures like BERT, NLI datasets are typically modeled via <em>sequence-pair classification</em>. That is, we feed both the premise and the hypothesis through the model together as distinct segments and learn a classification head predicting one of <code>[contradiction, neutral, entailment]</code>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="A-ready-made-zero-shot-classifier">A ready-made zero-shot classifier<a class="anchor-link" href="#A-ready-made-zero-shot-classifier"> </a></h3><p>Recently, <a href="https://arxiv.org/abs/1909.00161">Yin et al. (2019)</a> proposed a method which uses a pre-trained MNLI sequence-pair classifier as an out-of-the-box zero-shot text classifier that actually works pretty well.</p>
<p>The idea is to take the sequence we're interested in labeling as the "premise" and to turn each candidate label into a "hypothesis." If the model says that the premise "entails" the hypothesis, we take the label to be true. This gives us a ready-made compatibility function that works reasonably well on certain tasks without any task-specific training. See the code snippet below to see how easily this can be done with ðŸ¤— Transformers.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description" open="">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p><div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#collapse-show</span>
<span class="c1"># load model pretrained on MNLI</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">BartForSequenceClassification</span><span class="p">,</span> <span class="n">BartTokenizer</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BartTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bart-large-mnli&#39;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BartForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bart-large-mnli&#39;</span><span class="p">)</span>

<span class="c1"># pose sequence as a NLI premise and label (politics) as a hypothesis</span>
<span class="n">premise</span> <span class="o">=</span> <span class="s1">&#39;Who are you voting for in 2020?&#39;</span>
<span class="n">hypothesis</span> <span class="o">=</span> <span class="s1">&#39;This text is about politics.&#39;</span>

<span class="c1"># run through model pre-trained on MNLI</span>
<span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">premise</span><span class="p">,</span> <span class="n">hypothesis</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">&#39;pt&#39;</span><span class="p">)</span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># we throw away &quot;neutral&quot; (dim 1) and take the probability of</span>
<span class="c1"># &quot;entailment&quot; (2) as the probability of the label being true </span>
<span class="n">entail_contradiction_logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[:,[</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">]]</span>
<span class="n">probs</span> <span class="o">=</span> <span class="n">entail_contradiction_logits</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">true_prob</span> <span class="o">=</span> <span class="n">probs</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="mi">100</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Probability that the label is true: </span><span class="si">{</span><span class="n">true_prob</span><span class="si">:</span><span class="s1">0.2f</span><span class="si">}</span><span class="s1">%&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>
</p>
    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Probability that the label is true: 99.04%
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In the paper, the authors report an F1 of $37.9$ on Yahoo Answers using the smallest version of BERT fine-tuned only on the Multi-genre NLI (MNLI) corpus. By simply using the larger and more recent Bart model pre-trained on MNLI, we were able to bring this number up to $53.7$. For context, Yahoo Answers has 10 classes and <a href="https://paperswithcode.com/sota/text-classification-on-yahoo-answers">supervised models</a> get an accuracy of just over $70\%$.</p>
<p>Of course, this number can be improved when some data is available for training. In addition to the extreme fully unsupervised setting, the authors consider a setup which corresponds to the traditional <em>generalized zero-shot learning</em> setting where only a subset of the dataset's labels are available during training. The model is then evaluated on all labels together, both seen and unseen, at test time.</p>
<p>See <a href="http://35.208.71.201:8000/">our live demo here</a> to try it out for yourself! Enter a sequence you want to classify and any labels of interest and watch Bart do its magic in real time.</p>
<p><img src="https://i.ibb.co/WB6HsFk/Screen-Shot-2020-05-26-at-5-31-25-PM.png" alt="live demo" /></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="A-Latent-Embedding-Approach">A Latent Embedding Approach<a class="anchor-link" href="#A-Latent-Embedding-Approach"> </a></h3><p>A slightly less effective but more flexible approach is to embed both the sequence and the class names of interest into the same representation space and then simply select the label closest in latent space.</p>
<p>This is a well-known technique in Zero Shot Learning in Computer Vision. Take the word vectors for each class and some latent representation for an image and project them to the same space. Learning this projection requires data for some labels, but allows you to generalize to unseen labels at test time.</p>
<p>We found that in the text regime, we can follow the same procedure but without the need for any annotated data ahead of time. By simply using a single sentence representation model, we can embed both the sequences to classify and the candidate labels into the same latent space.</p>
$$
\hat{c} = \arg\max_{c \in C} \cos(\Phi(x), \Phi(c))
$$<p>where $\Phi$ is a sentence-level embedding model, $x$ is a sequence, and $C$ is a set of class labels.</p>
<p>Here's an example code snippet using the Sentence-BERT method:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># load the sentence-bert model from the HuggingFace model hub</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModel</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;deepset/sentence_bert&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;deepset/sentence_bert&quot;</span><span class="p">)</span>

<span class="n">sentence</span> <span class="o">=</span> <span class="s1">&#39;Who are you voting for in 2020?&#39;</span>
<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;politics&#39;</span><span class="p">,</span> <span class="s1">&#39;business&#39;</span><span class="p">,</span> <span class="s1">&#39;art &amp; culture&#39;</span><span class="p">]</span>

<span class="c1"># run inputs through model and mean over the sequence dimension</span>
<span class="c1"># to get sentence-level representations</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_encode_plus</span><span class="p">([</span><span class="n">sentence</span><span class="p">]</span> <span class="o">+</span> <span class="n">labels</span><span class="p">,</span>
                                     <span class="n">return_tensors</span><span class="o">=</span><span class="s1">&#39;pt&#39;</span><span class="p">,</span>
                                     <span class="n">pad_to_max_length</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">input_ids</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">]</span>
<span class="n">attention_mask</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="s1">&#39;attention_mask&#39;</span><span class="p">]</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">sentence_rep</span> <span class="o">=</span> <span class="n">output</span><span class="p">[:</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">label_reps</span> <span class="o">=</span> <span class="n">output</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># now find the labels with the highest cosine similarities to</span>
<span class="c1"># the sentence</span>
<span class="n">similarities</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cosine_similarity</span><span class="p">(</span><span class="n">sentence_rep</span><span class="p">,</span> <span class="n">label_reps</span><span class="p">)</span>
<span class="n">closest</span> <span class="o">=</span> <span class="n">similarities</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">descending</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">for</span> <span class="n">ind</span> <span class="ow">in</span> <span class="n">closest</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;label: </span><span class="si">{</span><span class="n">labels</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span><span class="si">}</span><span class="s1">,</span><span class="se">\t</span><span class="s1"> cos: </span><span class="si">{</span><span class="n">similarities</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span><span class="si">:</span><span class="s1">0.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>label: politics,	 cos: 0.2156
label: business,	 cos: 0.0045
label: art &amp; culture,	 cos: -0.0274
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>One downside to this method is that Sentence-BERT is designed to learn sentence-level, not single- or multi-word representations like our class names. It is therefore reasonable to suppose that our label embeddings may not be as semantically salient as popular word-level embedding methods. But we can't use word vectors directly because then we would have to align our S-BERT sequence embeddings with the word vector label emebddings.</p>
<p>We addressed this issue with the following procedure:</p>
<ol>
<li>Take the top $10,000$ most frequent words $V$ in skipgram's vocabulary</li>
<li>Obtain embeddings for each word using skipgram $\Phi_{\text{word}}(V)$</li>
<li>Obtain embeddings for each word using S-BERT $\Phi_{\text{sent}}(V)$</li>
<li>Learn a linear projection $W$ with L2 regularization from $\Phi_{\text{sent}}(V)$ to $\Phi_{\text{word}}(V)$</li>
</ol>
<p>Now we apply $W$ as an additional transformation to our latent space for both sequence and label embeddings:</p>
$$
\hat{c} = \arg\max_{c \in C} \cos(\Phi_{\text{sent}}(x)W, \Phi_{\text{sent}}(c)W)
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Setting:-Low-Resource-Languages">Setting: Low-Resource Languages<a class="anchor-link" href="#Setting:-Low-Resource-Languages"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Low-resource and cross-lingual learning is a huge research area in NLP right now and much has been written about it, so I'll just link a few great resources:</p>
<ul>
<li>Graham Neubig's recently released <a href="https://github.com/neubig/lowresource-nlp-bootcamp-2020">Low Resource NLP Bootcamp</a>.

<center>
    <div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">At <a href="https://twitter.com/LTIatCMU?ref_src=twsrc%5Etfw">@LTIatCMU</a> we held a week-long &quot;Low Resource Natural Language Processing Bootcamp&quot; with 8 sets of lectures &amp; exercises on getting NLP to work in languages where resources are less abundant. We&#39;re making them available for all who are interested here: <a href="https://t.co/GGc5tAM2lh">https://t.co/GGc5tAM2lh</a> 1/ <a href="https://t.co/oJOmiyTyCS">pic.twitter.com/oJOmiyTyCS</a></p>&mdash; Graham Neubig (@gneubig) <a href="https://twitter.com/gneubig/status/1265644923153514496?ref_src=twsrc%5Etfw">May 27, 2020</a></blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>
</center>
</li>
<li>Sebastian Ruder's blog post, <a href="https://ruder.io/cross-lingual-embeddings/">"A survey of cross-lingual word embedding models"</a></li>
</ul>

</div>
</div>
</div>
</div>


  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="joeddav/blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/blog/2020/05/28/ZSL.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Joe Davison</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Joe Davison</li><li><a class="u-email" href="mailto:josephddavison@gmail.com">josephddavison@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list">
  <li><a href="https://github.com/joeddav"><svg class="social svg-icon"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg> <span class="username">joeddav</span></a></li><li><a href="https://www.linkedin.com/in/josephdavison"><svg class="social svg-icon"><use xlink:href="/blog/assets/minima-social-icons.svg#linkedin"></use></svg> </a></li><li><a href="https://www.twitter.com/joeddav"><svg class="social svg-icon"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg> </a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Just a simple fastpages site for publishing things here and there</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
